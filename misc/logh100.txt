(venv) (base) root@jarvislabs:/home/practice_nanogpt# python train.py
we have the device : cuda
The number of parameters inside this transformer is 124475904
The number of parameters : 124.48M
didnt crash
total desired batch size:524288
calculated grad accum step:32
loaded 338024 tokens
1 epoch = 20 Batches
num decayed parameter tensors= 124354560
num non-decayed parameter tensors= 121344
using fused adamW: True
step: 0 ------> loss: 10.939375----------->Norm: 0.8277 -----> LR: 6.0000e-05   -------> dt: 2114.61ms--------> tokens/sec:247935.81
step: 1 ------> loss: 9.670183----------->Norm: 0.3127 -----> LR: 1.2000e-04   -------> dt: 1767.25ms--------> tokens/sec:296668.99
step: 2 ------> loss: 9.325103----------->Norm: 0.2940 -----> LR: 1.8000e-04   -------> dt: 1756.00ms--------> tokens/sec:298569.39
step: 3 ------> loss: 9.530610----------->Norm: 0.2021 -----> LR: 2.4000e-04   -------> dt: 1712.32ms--------> tokens/sec:306185.06
step: 4 ------> loss: 9.170056----------->Norm: 0.1445 -----> LR: 3.0000e-04   -------> dt: 1714.62ms--------> tokens/sec:305774.68
step: 5 ------> loss: 8.809067----------->Norm: 0.1158 -----> LR: 3.6000e-04   -------> dt: 1712.65ms--------> tokens/sec:306127.31
step: 6 ------> loss: 8.430701----------->Norm: 0.0988 -----> LR: 4.2000e-04   -------> dt: 1716.37ms--------> tokens/sec:305463.81
step: 7 ------> loss: 8.131289----------->Norm: 0.0627 -----> LR: 4.8000e-04   -------> dt: 1719.70ms--------> tokens/sec:304872.06
step: 8 ------> loss: 7.874549----------->Norm: 0.1115 -----> LR: 5.4000e-04   -------> dt: 1719.60ms--------> tokens/sec:304889.56
step: 9 ------> loss: 7.588320----------->Norm: 0.1222 -----> LR: 6.0000e-04   -------> dt: 1707.69ms--------> tokens/sec:307015.74
step: 10 ------> loss: 7.289483----------->Norm: 0.0759 -----> LR: 6.6000e-04   -------> dt: 1716.76ms--------> tokens/sec:305394.07
step: 11 ------> loss: 7.023554----------->Norm: 0.0790 -----> LR: 7.2000e-04   -------> dt: 1722.26ms--------> tokens/sec:304419.46
step: 12 ------> loss: 6.847145----------->Norm: 0.0881 -----> LR: 7.8000e-04   -------> dt: 1702.27ms--------> tokens/sec:307992.92
step: 13 ------> loss: 6.700529----------->Norm: 0.0695 -----> LR: 8.4000e-04   -------> dt: 1712.28ms--------> tokens/sec:306192.48
step: 14 ------> loss: 6.607672----------->Norm: 0.0785 -----> LR: 9.0000e-04   -------> dt: 1702.28ms--------> tokens/sec:307991.67
step: 15 ------> loss: 6.607171----------->Norm: 0.0684 -----> LR: 9.6000e-04   -------> dt: 1712.40ms--------> tokens/sec:306171.46
step: 16 ------> loss: 6.600137----------->Norm: 0.0678 -----> LR: 1.0200e-03   -------> dt: 1709.82ms--------> tokens/sec:306632.97
step: 17 ------> loss: 6.648515----------->Norm: 0.0614 -----> LR: 1.0800e-03   -------> dt: 1710.40ms--------> tokens/sec:306530.00
step: 18 ------> loss: 6.691835----------->Norm: 0.0512 -----> LR: 1.1400e-03   -------> dt: 1701.39ms--------> tokens/sec:308153.52
step: 19 ------> loss: 6.694038----------->Norm: 0.0622 -----> LR: 1.2000e-03   -------> dt: 1712.76ms--------> tokens/sec:306107.92
step: 20 ------> loss: 6.739795----------->Norm: 0.0594 -----> LR: 1.2600e-03   -------> dt: 1723.61ms--------> tokens/sec:304179.31
step: 21 ------> loss: 6.693457----------->Norm: 0.0535 -----> LR: 1.3200e-03   -------> dt: 1711.06ms--------> tokens/sec:306410.80
step: 22 ------> loss: 6.680285----------->Norm: 0.0498 -----> LR: 1.3800e-03   -------> dt: 1706.43ms--------> tokens/sec:307242.36
step: 23 ------> loss: 6.675347----------->Norm: 0.0806 -----> LR: 1.4400e-03   -------> dt: 1710.88ms--------> tokens/sec:306444.27
step: 24 ------> loss: 6.648803----------->Norm: 0.0793 -----> LR: 1.5000e-03   -------> dt: 1714.10ms--------> tokens/sec:305868.55
step: 25 ------> loss: 6.688380----------->Norm: 0.0609 -----> LR: 1.5600e-03   -------> dt: 1717.99ms--------> tokens/sec:305175.84
step: 26 ------> loss: 6.635276----------->Norm: 0.0388 -----> LR: 1.6200e-03   -------> dt: 1712.93ms--------> tokens/sec:306076.86
step: 27 ------> loss: 6.635262----------->Norm: 0.0275 -----> LR: 1.6800e-03   -------> dt: 1728.19ms--------> tokens/sec:303373.47
step: 28 ------> loss: 6.632571----------->Norm: 0.0324 -----> LR: 1.7400e-03   -------> dt: 1717.62ms--------> tokens/sec:305240.57
step: 29 ------> loss: 6.628331----------->Norm: 0.0479 -----> LR: 1.8000e-03   -------> dt: 1701.04ms--------> tokens/sec:308216.71
step: 30 ------> loss: 6.675426----------->Norm: 0.0407 -----> LR: 1.8600e-03   -------> dt: 1712.67ms--------> tokens/sec:306122.96
step: 31 ------> loss: 6.609492----------->Norm: 0.0348 -----> LR: 1.9200e-03   -------> dt: 1735.93ms--------> tokens/sec:302022.02
step: 32 ------> loss: 6.584578----------->Norm: 0.0434 -----> LR: 1.9800e-03   -------> dt: 1714.54ms--------> tokens/sec:305789.77
step: 33 ------> loss: 6.630918----------->Norm: 0.0303 -----> LR: 2.0400e-03   -------> dt: 1708.80ms--------> tokens/sec:306816.29
step: 34 ------> loss: 6.570713----------->Norm: 0.0366 -----> LR: 2.1000e-03   -------> dt: 1727.42ms--------> tokens/sec:303509.35
step: 35 ------> loss: 6.604161----------->Norm: 0.0475 -----> LR: 2.1600e-03   -------> dt: 1708.01ms--------> tokens/sec:306958.61
step: 36 ------> loss: 6.577800----------->Norm: 0.0348 -----> LR: 2.2200e-03   -------> dt: 1699.95ms--------> tokens/sec:308414.21
step: 37 ------> loss: 6.587290----------->Norm: 0.0327 -----> LR: 2.2800e-03   -------> dt: 1714.65ms--------> tokens/sec:305769.75
step: 38 ------> loss: 6.573550----------->Norm: 0.0504 -----> LR: 2.3400e-03   -------> dt: 1705.85ms--------> tokens/sec:307347.95
step: 39 ------> loss: 6.606481----------->Norm: 0.0423 -----> LR: 2.4000e-03   -------> dt: 1719.30ms--------> tokens/sec:304943.00
step: 40 ------> loss: 6.685697----------->Norm: 0.0388 -----> LR: 2.4600e-03   -------> dt: 1740.22ms--------> tokens/sec:301276.55
step: 41 ------> loss: 6.556426----------->Norm: 0.0223 -----> LR: 2.5200e-03   -------> dt: 1711.72ms--------> tokens/sec:306293.64
step: 42 ------> loss: 6.526474----------->Norm: 0.0312 -----> LR: 2.5800e-03   -------> dt: 1709.63ms--------> tokens/sec:306667.74
step: 43 ------> loss: 6.522655----------->Norm: 0.0304 -----> LR: 2.6400e-03   -------> dt: 1715.72ms--------> tokens/sec:305578.46
step: 44 ------> loss: 6.489003----------->Norm: 0.0343 -----> LR: 2.7000e-03   -------> dt: 1705.65ms--------> tokens/sec:307383.01
step: 45 ------> loss: 6.573148----------->Norm: 0.0353 -----> LR: 2.7600e-03   -------> dt: 1720.72ms--------> tokens/sec:304691.05
step: 46 ------> loss: 6.534676----------->Norm: 0.0373 -----> LR: 2.8200e-03   -------> dt: 1729.39ms--------> tokens/sec:303163.48
step: 47 ------> loss: 6.519935----------->Norm: 0.0196 -----> LR: 2.8800e-03   -------> dt: 1718.89ms--------> tokens/sec:305016.00
step: 48 ------> loss: 6.497702----------->Norm: 0.0234 -----> LR: 2.9400e-03   -------> dt: 1710.31ms--------> tokens/sec:306545.22
step: 49 ------> loss: 6.469082----------->Norm: 0.0329 -----> LR: 3.0000e-03   -------> dt: 1730.85ms--------> tokens/sec:302907.70