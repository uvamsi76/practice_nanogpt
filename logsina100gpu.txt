(venv) root@18a2565cd40f:~/project-llm101/practice_nanogpt# python train.py
we have the device : cuda
The number of parameters inside this transformer is 124439808
The number of parameters : 124.44M
didnt crash
loaded 338024 tokens
1 epoch = 20 Batches
step: 0 ------> loss: 10.92485237121582------------> dt: 1315.43ms--------> tokens/sec:194.61
step: 1 ------> loss: 9.422855377197266------------> dt: 1082.50ms--------> tokens/sec:236.49
step: 2 ------> loss: 9.023154258728027------------> dt: 1085.70ms--------> tokens/sec:235.79
step: 3 ------> loss: 8.922901153564453------------> dt: 1085.99ms--------> tokens/sec:235.73
step: 4 ------> loss: 8.615105628967285------------> dt: 1085.94ms--------> tokens/sec:235.74
step: 5 ------> loss: 8.487377166748047------------> dt: 1085.19ms--------> tokens/sec:235.90
step: 6 ------> loss: 8.3231782913208------------> dt: 1087.14ms--------> tokens/sec:235.48
step: 7 ------> loss: 8.125436782836914------------> dt: 1088.45ms--------> tokens/sec:235.20
step: 8 ------> loss: 7.847939491271973------------> dt: 1085.22ms--------> tokens/sec:235.90
step: 9 ------> loss: 7.602139472961426------------> dt: 1087.12ms--------> tokens/sec:235.48
step: 10 ------> loss: 7.424348831176758------------> dt: 1089.10ms--------> tokens/sec:235.06
step: 11 ------> loss: 7.306183338165283------------> dt: 1086.54ms--------> tokens/sec:235.61
step: 12 ------> loss: 7.142875671386719------------> dt: 1088.64ms--------> tokens/sec:235.15
step: 13 ------> loss: 7.072482109069824------------> dt: 1090.57ms--------> tokens/sec:234.74
step: 14 ------> loss: 7.003154754638672------------> dt: 1087.07ms--------> tokens/sec:235.50
step: 15 ------> loss: 6.850156307220459------------> dt: 1091.81ms--------> tokens/sec:234.47
step: 16 ------> loss: 6.819701194763184------------> dt: 1090.92ms--------> tokens/sec:234.66
step: 17 ------> loss: 6.802680969238281------------> dt: 1088.91ms--------> tokens/sec:235.10
step: 18 ------> loss: 6.797522068023682------------> dt: 1090.92ms--------> tokens/sec:234.66
step: 19 ------> loss: 6.632956027984619------------> dt: 1092.12ms--------> tokens/sec:234.41
step: 20 ------> loss: 6.531059741973877------------> dt: 1091.95ms--------> tokens/sec:234.44
step: 21 ------> loss: 6.323575496673584------------> dt: 1093.62ms--------> tokens/sec:234.08
step: 22 ------> loss: 6.383141994476318------------> dt: 1096.03ms--------> tokens/sec:233.57
step: 23 ------> loss: 6.320435523986816------------> dt: 1095.48ms--------> tokens/sec:233.69
step: 24 ------> loss: 6.258792877197266------------> dt: 1096.09ms--------> tokens/sec:233.56
step: 25 ------> loss: 6.472315311431885------------> dt: 1097.62ms--------> tokens/sec:233.23
step: 26 ------> loss: 6.550581932067871------------> dt: 1096.82ms--------> tokens/sec:233.40
step: 27 ------> loss: 6.438724040985107------------> dt: 1096.99ms--------> tokens/sec:233.37
step: 28 ------> loss: 6.356234073638916------------> dt: 1097.58ms--------> tokens/sec:233.24
step: 29 ------> loss: 6.254861831665039------------> dt: 1098.83ms--------> tokens/sec:232.98
step: 30 ------> loss: 6.2624406814575195------------> dt: 1097.61ms--------> tokens/sec:233.23
step: 31 ------> loss: 6.33498477935791------------> dt: 1100.40ms--------> tokens/sec:232.64
step: 32 ------> loss: 6.2432637214660645------------> dt: 1100.99ms--------> tokens/sec:232.52
step: 33 ------> loss: 6.374934196472168------------> dt: 1099.77ms--------> tokens/sec:232.78
step: 34 ------> loss: 6.468234539031982------------> dt: 1098.92ms--------> tokens/sec:232.96
step: 35 ------> loss: 6.309868335723877------------> dt: 1098.99ms--------> tokens/sec:232.94
step: 36 ------> loss: 6.309178829193115------------> dt: 1099.60ms--------> tokens/sec:232.81
step: 37 ------> loss: 6.320328712463379------------> dt: 1102.60ms--------> tokens/sec:232.18
step: 38 ------> loss: 6.310102939605713------------> dt: 1102.91ms--------> tokens/sec:232.11
step: 39 ------> loss: 6.166642189025879------------> dt: 1104.34ms--------> tokens/sec:231.81
step: 40 ------> loss: 6.306861400604248------------> dt: 1101.72ms--------> tokens/sec:232.36
step: 41 ------> loss: 6.062314987182617------------> dt: 1104.30ms--------> tokens/sec:231.82
step: 42 ------> loss: 6.2028679847717285------------> dt: 1104.79ms--------> tokens/sec:231.72
step: 43 ------> loss: 6.102695941925049------------> dt: 1103.67ms--------> tokens/sec:231.95
step: 44 ------> loss: 6.043275833129883------------> dt: 1105.04ms--------> tokens/sec:231.67
step: 45 ------> loss: 6.262006759643555------------> dt: 1102.76ms--------> tokens/sec:232.14
step: 46 ------> loss: 6.377468585968018------------> dt: 1108.74ms--------> tokens/sec:230.89
step: 47 ------> loss: 6.251377582550049------------> dt: 1103.50ms--------> tokens/sec:231.99
step: 48 ------> loss: 6.187267780303955------------> dt: 1107.17ms--------> tokens/sec:231.22
step: 49 ------> loss: 6.102104663848877------------> dt: 1106.26ms--------> tokens/sec:231.41
(venv) root@18a2565cd40f:~/project-llm101/practice_nanogpt# python train.py
we have the device : cuda
The number of parameters inside this transformer is 124439808
The number of parameters : 124.44M
didnt crash
loaded 338024 tokens
1 epoch = 20 Batches
step: 0 ------> loss: 10.924817085266113------------> dt: 647.82ms--------> tokens/sec:395.17
step: 1 ------> loss: 9.422725677490234------------> dt: 396.21ms--------> tokens/sec:646.13
step: 2 ------> loss: 9.022689819335938------------> dt: 396.36ms--------> tokens/sec:645.88
step: 3 ------> loss: 8.922737121582031------------> dt: 397.64ms--------> tokens/sec:643.79
step: 4 ------> loss: 8.614474296569824------------> dt: 396.26ms--------> tokens/sec:646.05
step: 5 ------> loss: 8.486926078796387------------> dt: 396.29ms--------> tokens/sec:646.00
step: 6 ------> loss: 8.323283195495605------------> dt: 396.63ms--------> tokens/sec:645.44
step: 7 ------> loss: 8.12548542022705------------> dt: 397.25ms--------> tokens/sec:644.44
step: 8 ------> loss: 7.847541332244873------------> dt: 396.77ms--------> tokens/sec:645.20
step: 9 ------> loss: 7.601734161376953------------> dt: 396.62ms--------> tokens/sec:645.45
step: 10 ------> loss: 7.424289226531982------------> dt: 397.17ms--------> tokens/sec:644.57
step: 11 ------> loss: 7.306268692016602------------> dt: 397.02ms--------> tokens/sec:644.80
step: 12 ------> loss: 7.142860412597656------------> dt: 396.94ms--------> tokens/sec:644.93
step: 13 ------> loss: 7.072248458862305------------> dt: 396.78ms--------> tokens/sec:645.19
step: 14 ------> loss: 7.002817630767822------------> dt: 397.56ms--------> tokens/sec:643.92
step: 15 ------> loss: 6.849835395812988------------> dt: 397.07ms--------> tokens/sec:644.72
step: 16 ------> loss: 6.8196120262146------------> dt: 397.14ms--------> tokens/sec:644.61
step: 17 ------> loss: 6.802721977233887------------> dt: 396.99ms--------> tokens/sec:644.86
step: 18 ------> loss: 6.797419548034668------------> dt: 397.10ms--------> tokens/sec:644.68
step: 19 ------> loss: 6.632590293884277------------> dt: 396.97ms--------> tokens/sec:644.88
step: 20 ------> loss: 6.530766487121582------------> dt: 397.29ms--------> tokens/sec:644.37
step: 21 ------> loss: 6.323502540588379------------> dt: 397.22ms--------> tokens/sec:644.47
step: 22 ------> loss: 6.382964611053467------------> dt: 397.37ms--------> tokens/sec:644.24
step: 23 ------> loss: 6.319913387298584------------> dt: 398.15ms--------> tokens/sec:642.98
step: 24 ------> loss: 6.258100509643555------------> dt: 397.32ms--------> tokens/sec:644.31
step: 25 ------> loss: 6.471643447875977------------> dt: 396.76ms--------> tokens/sec:645.23
step: 26 ------> loss: 6.550081729888916------------> dt: 398.11ms--------> tokens/sec:643.05
step: 27 ------> loss: 6.438286781311035------------> dt: 397.73ms--------> tokens/sec:643.65
step: 28 ------> loss: 6.355838298797607------------> dt: 398.58ms--------> tokens/sec:642.29
step: 29 ------> loss: 6.254594326019287------------> dt: 398.85ms--------> tokens/sec:641.84
step: 30 ------> loss: 6.2621378898620605------------> dt: 397.92ms--------> tokens/sec:643.35
step: 31 ------> loss: 6.312685489654541------------> dt: 397.92ms--------> tokens/sec:643.35
step: 32 ------> loss: 6.242242813110352------------> dt: 398.18ms--------> tokens/sec:642.93
step: 33 ------> loss: 6.382919788360596------------> dt: 398.08ms--------> tokens/sec:643.09
step: 34 ------> loss: 6.469964027404785------------> dt: 398.07ms--------> tokens/sec:643.10
step: 35 ------> loss: 6.3084001541137695------------> dt: 397.92ms--------> tokens/sec:643.34
step: 36 ------> loss: 6.306249141693115------------> dt: 398.42ms--------> tokens/sec:642.54
step: 37 ------> loss: 6.31179141998291------------> dt: 398.37ms--------> tokens/sec:642.62
step: 38 ------> loss: 6.302652359008789------------> dt: 398.10ms--------> tokens/sec:643.06
step: 39 ------> loss: 6.160470485687256------------> dt: 397.94ms--------> tokens/sec:643.31
step: 40 ------> loss: 6.303842544555664------------> dt: 398.57ms--------> tokens/sec:642.29
step: 41 ------> loss: 6.058374881744385------------> dt: 397.73ms--------> tokens/sec:643.65
step: 42 ------> loss: 6.196630477905273------------> dt: 398.26ms--------> tokens/sec:642.80
step: 43 ------> loss: 6.0915045738220215------------> dt: 398.95ms--------> tokens/sec:641.68
step: 44 ------> loss: 6.031656742095947------------> dt: 398.24ms--------> tokens/sec:642.83
step: 45 ------> loss: 6.253142356872559------------> dt: 397.81ms--------> tokens/sec:643.52
step: 46 ------> loss: 6.370029926300049------------> dt: 398.64ms--------> tokens/sec:642.18
step: 47 ------> loss: 6.242198944091797------------> dt: 398.98ms--------> tokens/sec:641.64
step: 48 ------> loss: 6.178633689880371------------> dt: 398.72ms--------> tokens/sec:642.06
step: 49 ------> loss: 6.0954508781433105------------> dt: 399.96ms--------> tokens/sec:640.07
(venv) root@18a2565cd40f:~/project-llm101/practice_nanogpt# python train.py
we have the device : cuda
The number of parameters inside this transformer is 124439808
The number of parameters : 124.44M
didnt crash
loaded 338024 tokens
1 epoch = 20 Batches
step: 0 ------> loss: 10.92535400390625------------> dt: 680.47ms--------> tokens/sec:376.21
step: 1 ------> loss: 9.423076629638672------------> dt: 340.57ms--------> tokens/sec:751.67
step: 2 ------> loss: 9.025083541870117------------> dt: 340.05ms--------> tokens/sec:752.82
step: 3 ------> loss: 8.924705505371094------------> dt: 340.27ms--------> tokens/sec:752.34
step: 4 ------> loss: 8.616661071777344------------> dt: 342.55ms--------> tokens/sec:747.33
step: 5 ------> loss: 8.488821029663086------------> dt: 341.72ms--------> tokens/sec:749.15
step: 6 ------> loss: 8.3226318359375------------> dt: 340.87ms--------> tokens/sec:751.03
step: 7 ------> loss: 8.125503540039062------------> dt: 339.96ms--------> tokens/sec:753.03
step: 8 ------> loss: 7.846469879150391------------> dt: 340.75ms--------> tokens/sec:751.29
step: 9 ------> loss: 7.604034423828125------------> dt: 340.71ms--------> tokens/sec:751.38
step: 10 ------> loss: 7.423577308654785------------> dt: 340.78ms--------> tokens/sec:751.22
step: 11 ------> loss: 7.306781768798828------------> dt: 340.32ms--------> tokens/sec:752.23
step: 12 ------> loss: 7.142692565917969------------> dt: 341.00ms--------> tokens/sec:750.73
step: 13 ------> loss: 7.072993278503418------------> dt: 341.85ms--------> tokens/sec:748.87
step: 14 ------> loss: 7.003543853759766------------> dt: 340.37ms--------> tokens/sec:752.12
step: 15 ------> loss: 6.849977493286133------------> dt: 340.97ms--------> tokens/sec:750.79
step: 16 ------> loss: 6.820639610290527------------> dt: 340.62ms--------> tokens/sec:751.56
step: 17 ------> loss: 6.801360607147217------------> dt: 341.05ms--------> tokens/sec:750.61
step: 18 ------> loss: 6.798953533172607------------> dt: 340.47ms--------> tokens/sec:751.91
step: 19 ------> loss: 6.633533954620361------------> dt: 340.88ms--------> tokens/sec:751.01
step: 20 ------> loss: 6.531637668609619------------> dt: 342.07ms--------> tokens/sec:748.39
step: 21 ------> loss: 6.324845790863037------------> dt: 340.93ms--------> tokens/sec:750.88
step: 22 ------> loss: 6.384276866912842------------> dt: 341.45ms--------> tokens/sec:749.75
step: 23 ------> loss: 6.322660446166992------------> dt: 343.37ms--------> tokens/sec:745.54
step: 24 ------> loss: 6.2601518630981445------------> dt: 342.25ms--------> tokens/sec:747.99
step: 25 ------> loss: 6.473627090454102------------> dt: 343.53ms--------> tokens/sec:745.20
step: 26 ------> loss: 6.552128314971924------------> dt: 343.68ms--------> tokens/sec:744.88
step: 27 ------> loss: 6.4413251876831055------------> dt: 341.12ms--------> tokens/sec:750.47
step: 28 ------> loss: 6.358335971832275------------> dt: 341.73ms--------> tokens/sec:749.13
step: 29 ------> loss: 6.254742622375488------------> dt: 341.86ms--------> tokens/sec:748.85
step: 30 ------> loss: 6.262265682220459------------> dt: 343.35ms--------> tokens/sec:745.59
step: 31 ------> loss: 6.333810806274414------------> dt: 341.91ms--------> tokens/sec:748.73
step: 32 ------> loss: 6.241766929626465------------> dt: 341.50ms--------> tokens/sec:749.64
step: 33 ------> loss: 6.364738464355469------------> dt: 342.99ms--------> tokens/sec:746.39
step: 34 ------> loss: 6.46827507019043------------> dt: 341.44ms--------> tokens/sec:749.76
step: 35 ------> loss: 6.306997776031494------------> dt: 341.06ms--------> tokens/sec:750.60
step: 36 ------> loss: 6.304235458374023------------> dt: 340.72ms--------> tokens/sec:751.35
step: 37 ------> loss: 6.313804626464844------------> dt: 342.23ms--------> tokens/sec:748.02
step: 38 ------> loss: 6.30123233795166------------> dt: 341.65ms--------> tokens/sec:749.30
step: 39 ------> loss: 6.157658576965332------------> dt: 342.24ms--------> tokens/sec:748.01
step: 40 ------> loss: 6.3030571937561035------------> dt: 341.72ms--------> tokens/sec:749.16
step: 41 ------> loss: 6.061041831970215------------> dt: 341.94ms--------> tokens/sec:748.67
step: 42 ------> loss: 6.203486919403076------------> dt: 341.55ms--------> tokens/sec:749.52
step: 43 ------> loss: 6.0926947593688965------------> dt: 342.78ms--------> tokens/sec:746.84
step: 44 ------> loss: 6.036599159240723------------> dt: 341.97ms--------> tokens/sec:748.59
step: 45 ------> loss: 6.261441230773926------------> dt: 341.96ms--------> tokens/sec:748.62
step: 46 ------> loss: 6.38136100769043------------> dt: 341.53ms--------> tokens/sec:749.57
step: 47 ------> loss: 6.2538228034973145------------> dt: 341.81ms--------> tokens/sec:748.95
step: 48 ------> loss: 6.189001083374023------------> dt: 342.49ms--------> tokens/sec:747.46
step: 49 ------> loss: 6.105751037597656------------> dt: 342.22ms--------> tokens/sec:748.07
(venv) root@18a2565cd40f:~/project-llm101/practice_nanogpt# python train.py
we have the device : cuda
The number of parameters inside this transformer is 124439808
The number of parameters : 124.44M
didnt crash
loaded 338024 tokens
1 epoch = 20 Batches
step: 0 ------> loss: 10.925289154052734------------> dt: 31398.32ms--------> tokens/sec:8.15
step: 1 ------> loss: 9.422700881958008------------> dt: 148.17ms--------> tokens/sec:1727.70
step: 2 ------> loss: 9.023154258728027------------> dt: 148.78ms--------> tokens/sec:1720.66
step: 3 ------> loss: 8.923084259033203------------> dt: 149.29ms--------> tokens/sec:1714.82
step: 4 ------> loss: 8.614778518676758------------> dt: 149.31ms--------> tokens/sec:1714.50
step: 5 ------> loss: 8.48737621307373------------> dt: 147.21ms--------> tokens/sec:1739.03
step: 6 ------> loss: 8.324565887451172------------> dt: 148.20ms--------> tokens/sec:1727.41
step: 7 ------> loss: 8.12710952758789------------> dt: 148.19ms--------> tokens/sec:1727.48
step: 8 ------> loss: 7.847689628601074------------> dt: 149.55ms--------> tokens/sec:1711.78
step: 9 ------> loss: 7.602548599243164------------> dt: 148.28ms--------> tokens/sec:1726.43
step: 10 ------> loss: 7.424798011779785------------> dt: 148.27ms--------> tokens/sec:1726.59
step: 11 ------> loss: 7.306648254394531------------> dt: 149.35ms--------> tokens/sec:1714.08
step: 12 ------> loss: 7.143426895141602------------> dt: 147.39ms--------> tokens/sec:1736.91
step: 13 ------> loss: 7.0724334716796875------------> dt: 149.03ms--------> tokens/sec:1717.74
step: 14 ------> loss: 7.0034074783325195------------> dt: 148.38ms--------> tokens/sec:1725.30
step: 15 ------> loss: 6.849825382232666------------> dt: 148.30ms--------> tokens/sec:1726.24
step: 16 ------> loss: 6.819800853729248------------> dt: 148.64ms--------> tokens/sec:1722.33
step: 17 ------> loss: 6.8028788566589355------------> dt: 148.43ms--------> tokens/sec:1724.70
step: 18 ------> loss: 6.7975616455078125------------> dt: 147.94ms--------> tokens/sec:1730.39
step: 19 ------> loss: 6.63313627243042------------> dt: 148.44ms--------> tokens/sec:1724.55
step: 20 ------> loss: 6.530719757080078------------> dt: 148.48ms--------> tokens/sec:1724.12
step: 21 ------> loss: 6.323447227478027------------> dt: 148.52ms--------> tokens/sec:1723.63
step: 22 ------> loss: 6.382899284362793------------> dt: 148.64ms--------> tokens/sec:1722.23
step: 23 ------> loss: 6.320271968841553------------> dt: 148.57ms--------> tokens/sec:1723.10
step: 24 ------> loss: 6.258385181427002------------> dt: 148.25ms--------> tokens/sec:1726.82
step: 25 ------> loss: 6.471715927124023------------> dt: 149.66ms--------> tokens/sec:1710.60
step: 26 ------> loss: 6.550179481506348------------> dt: 148.03ms--------> tokens/sec:1729.39
step: 27 ------> loss: 6.438457489013672------------> dt: 148.31ms--------> tokens/sec:1726.16
step: 28 ------> loss: 6.355682373046875------------> dt: 150.17ms--------> tokens/sec:1704.76
step: 29 ------> loss: 6.254631042480469------------> dt: 148.75ms--------> tokens/sec:1720.97
step: 30 ------> loss: 6.261995315551758------------> dt: 148.66ms--------> tokens/sec:1722.02
step: 31 ------> loss: 6.3336663246154785------------> dt: 149.87ms--------> tokens/sec:1708.14
step: 32 ------> loss: 6.257393836975098------------> dt: 148.18ms--------> tokens/sec:1727.64
step: 33 ------> loss: 6.356167793273926------------> dt: 148.93ms--------> tokens/sec:1718.98
step: 34 ------> loss: 6.466265678405762------------> dt: 149.58ms--------> tokens/sec:1711.42
step: 35 ------> loss: 6.317187786102295------------> dt: 149.15ms--------> tokens/sec:1716.34
step: 36 ------> loss: 6.313218116760254------------> dt: 149.54ms--------> tokens/sec:1711.94
step: 37 ------> loss: 6.317912578582764------------> dt: 149.05ms--------> tokens/sec:1717.55
step: 38 ------> loss: 6.306181907653809------------> dt: 148.18ms--------> tokens/sec:1727.58
step: 39 ------> loss: 6.16462516784668------------> dt: 149.56ms--------> tokens/sec:1711.73
step: 40 ------> loss: 6.3111042976379395------------> dt: 148.60ms--------> tokens/sec:1722.80
step: 41 ------> loss: 6.072911262512207------------> dt: 147.98ms--------> tokens/sec:1729.96
step: 42 ------> loss: 6.206048011779785------------> dt: 150.07ms--------> tokens/sec:1705.87
step: 43 ------> loss: 6.10310173034668------------> dt: 148.83ms--------> tokens/sec:1720.12
step: 44 ------> loss: 6.0514020919799805------------> dt: 148.50ms--------> tokens/sec:1723.88
step: 45 ------> loss: 6.272277355194092------------> dt: 150.32ms--------> tokens/sec:1703.07
step: 46 ------> loss: 6.387600898742676------------> dt: 148.72ms--------> tokens/sec:1721.34
step: 47 ------> loss: 6.25931453704834------------> dt: 149.25ms--------> tokens/sec:1715.29
step: 48 ------> loss: 6.198882102966309------------> dt: 149.46ms--------> tokens/sec:1712.87
step: 49 ------> loss: 6.111042022705078------------> dt: 149.11ms--------> tokens/sec:1716.89
(venv) root@18a2565cd40f:~/project-llm101/practice_nanogpt# python train.py
we have the device : cuda
The number of parameters inside this transformer is 124439808
The number of parameters : 124.44M
didnt crash
loaded 338024 tokens
1 epoch = 20 Batches
step: 0 ------> loss: 10.925272941589355------------> dt: 20683.12ms--------> tokens/sec:12.38
step: 1 ------> loss: 9.422754287719727------------> dt: 105.02ms--------> tokens/sec:2437.69
step: 2 ------> loss: 9.023120880126953------------> dt: 106.66ms--------> tokens/sec:2400.21
step: 3 ------> loss: 8.923086166381836------------> dt: 107.63ms--------> tokens/sec:2378.59
step: 4 ------> loss: 8.614641189575195------------> dt: 107.80ms--------> tokens/sec:2374.66
step: 5 ------> loss: 8.48736572265625------------> dt: 105.89ms--------> tokens/sec:2417.71
step: 6 ------> loss: 8.324602127075195------------> dt: 106.56ms--------> tokens/sec:2402.38
step: 7 ------> loss: 8.127175331115723------------> dt: 107.25ms--------> tokens/sec:2386.87
step: 8 ------> loss: 7.847743511199951------------> dt: 106.06ms--------> tokens/sec:2413.77
step: 9 ------> loss: 7.602555274963379------------> dt: 106.65ms--------> tokens/sec:2400.43
step: 10 ------> loss: 7.424846172332764------------> dt: 106.30ms--------> tokens/sec:2408.30
step: 11 ------> loss: 7.306632041931152------------> dt: 107.23ms--------> tokens/sec:2387.41
step: 12 ------> loss: 7.143500328063965------------> dt: 106.88ms--------> tokens/sec:2395.17
step: 13 ------> loss: 7.072512626647949------------> dt: 106.41ms--------> tokens/sec:2405.68
step: 14 ------> loss: 7.00343132019043------------> dt: 106.74ms--------> tokens/sec:2398.30
step: 15 ------> loss: 6.849809646606445------------> dt: 108.29ms--------> tokens/sec:2363.99
step: 16 ------> loss: 6.819831848144531------------> dt: 105.23ms--------> tokens/sec:2432.75
step: 17 ------> loss: 6.802980422973633------------> dt: 105.19ms--------> tokens/sec:2433.61
step: 18 ------> loss: 6.797625541687012------------> dt: 105.96ms--------> tokens/sec:2416.07
step: 19 ------> loss: 6.6332268714904785------------> dt: 106.22ms--------> tokens/sec:2410.15
step: 20 ------> loss: 6.530762195587158------------> dt: 106.26ms--------> tokens/sec:2409.21
step: 21 ------> loss: 6.323637962341309------------> dt: 105.67ms--------> tokens/sec:2422.59
step: 22 ------> loss: 6.383059501647949------------> dt: 105.47ms--------> tokens/sec:2427.30
step: 23 ------> loss: 6.320490837097168------------> dt: 106.31ms--------> tokens/sec:2408.14
step: 24 ------> loss: 6.258775234222412------------> dt: 107.69ms--------> tokens/sec:2377.13
step: 25 ------> loss: 6.471926212310791------------> dt: 105.92ms--------> tokens/sec:2416.82
step: 26 ------> loss: 6.550359725952148------------> dt: 105.96ms--------> tokens/sec:2416.06
step: 27 ------> loss: 6.438518524169922------------> dt: 106.37ms--------> tokens/sec:2406.78
step: 28 ------> loss: 6.355890274047852------------> dt: 107.37ms--------> tokens/sec:2384.29
step: 29 ------> loss: 6.254660606384277------------> dt: 105.58ms--------> tokens/sec:2424.74
step: 30 ------> loss: 6.26219367980957------------> dt: 105.64ms--------> tokens/sec:2423.34
step: 31 ------> loss: 6.3351545333862305------------> dt: 105.62ms--------> tokens/sec:2423.74
step: 32 ------> loss: 6.242408752441406------------> dt: 106.86ms--------> tokens/sec:2395.75
step: 33 ------> loss: 6.376559257507324------------> dt: 107.20ms--------> tokens/sec:2387.99
step: 34 ------> loss: 6.468021392822266------------> dt: 105.84ms--------> tokens/sec:2418.84
step: 35 ------> loss: 6.306333541870117------------> dt: 105.52ms--------> tokens/sec:2426.18
step: 36 ------> loss: 6.305787563323975------------> dt: 106.75ms--------> tokens/sec:2398.08
step: 37 ------> loss: 6.316197395324707------------> dt: 107.76ms--------> tokens/sec:2375.75
step: 38 ------> loss: 6.304946422576904------------> dt: 105.72ms--------> tokens/sec:2421.55
step: 39 ------> loss: 6.160597801208496------------> dt: 105.76ms--------> tokens/sec:2420.58
step: 40 ------> loss: 6.30461311340332------------> dt: 106.58ms--------> tokens/sec:2401.90
step: 41 ------> loss: 6.061524391174316------------> dt: 107.25ms--------> tokens/sec:2386.84
step: 42 ------> loss: 6.201277732849121------------> dt: 105.97ms--------> tokens/sec:2415.82
step: 43 ------> loss: 6.09799861907959------------> dt: 105.98ms--------> tokens/sec:2415.61
step: 44 ------> loss: 6.037088871002197------------> dt: 106.15ms--------> tokens/sec:2411.67
step: 45 ------> loss: 6.257122039794922------------> dt: 107.82ms--------> tokens/sec:2374.30
step: 46 ------> loss: 6.373456001281738------------> dt: 106.24ms--------> tokens/sec:2409.61
step: 47 ------> loss: 6.24700927734375------------> dt: 106.32ms--------> tokens/sec:2407.73
step: 48 ------> loss: 6.181319713592529------------> dt: 106.66ms--------> tokens/sec:2400.16
step: 49 ------> loss: 6.097125053405762------------> dt: 107.52ms--------> tokens/sec:2380.93
(venv) root@18a2565cd40f:~/project-llm101/practice_nanogpt# python train.py
we have the device : cuda
The number of parameters inside this transformer is 124475904
The number of parameters : 124.48M
didnt crash
loaded 338024 tokens
1 epoch = 20 Batches
step: 0 ------> loss: 10.950811386108398------------> dt: 19851.68ms--------> tokens/sec:12.90
step: 1 ------> loss: 9.39072036743164------------> dt: 102.08ms--------> tokens/sec:2507.95
step: 2 ------> loss: 8.958845138549805------------> dt: 103.81ms--------> tokens/sec:2466.13
step: 3 ------> loss: 8.849885940551758------------> dt: 103.78ms--------> tokens/sec:2466.82
step: 4 ------> loss: 8.500955581665039------------> dt: 102.26ms--------> tokens/sec:2503.42
step: 5 ------> loss: 8.476860046386719------------> dt: 101.51ms--------> tokens/sec:2521.85
step: 6 ------> loss: 8.316941261291504------------> dt: 101.29ms--------> tokens/sec:2527.35
step: 7 ------> loss: 8.07803726196289------------> dt: 102.04ms--------> tokens/sec:2508.84
step: 8 ------> loss: 7.822882175445557------------> dt: 102.15ms--------> tokens/sec:2506.09
step: 9 ------> loss: 7.5944929122924805------------> dt: 102.31ms--------> tokens/sec:2502.15
step: 10 ------> loss: 7.396662712097168------------> dt: 102.98ms--------> tokens/sec:2485.94
step: 11 ------> loss: 7.286109924316406------------> dt: 101.79ms--------> tokens/sec:2514.97
step: 12 ------> loss: 7.084626197814941------------> dt: 101.97ms--------> tokens/sec:2510.60
step: 13 ------> loss: 7.02004337310791------------> dt: 102.14ms--------> tokens/sec:2506.34
step: 14 ------> loss: 6.974724769592285------------> dt: 102.56ms--------> tokens/sec:2496.06
step: 15 ------> loss: 6.790302276611328------------> dt: 103.58ms--------> tokens/sec:2471.58
step: 16 ------> loss: 6.739693641662598------------> dt: 102.57ms--------> tokens/sec:2495.98
step: 17 ------> loss: 6.686842918395996------------> dt: 102.85ms--------> tokens/sec:2488.95
step: 18 ------> loss: 6.640056610107422------------> dt: 103.77ms--------> tokens/sec:2467.07
step: 19 ------> loss: 6.4731597900390625------------> dt: 103.42ms--------> tokens/sec:2475.35
step: 20 ------> loss: 6.487738132476807------------> dt: 103.71ms--------> tokens/sec:2468.50
step: 21 ------> loss: 6.230103015899658------------> dt: 103.03ms--------> tokens/sec:2484.65
step: 22 ------> loss: 6.300101280212402------------> dt: 102.29ms--------> tokens/sec:2502.71
step: 23 ------> loss: 6.251837730407715------------> dt: 102.43ms--------> tokens/sec:2499.18
step: 24 ------> loss: 6.181556701660156------------> dt: 102.02ms--------> tokens/sec:2509.35
step: 25 ------> loss: 6.410505771636963------------> dt: 102.06ms--------> tokens/sec:2508.34
step: 26 ------> loss: 6.49200439453125------------> dt: 102.45ms--------> tokens/sec:2498.77
step: 27 ------> loss: 6.3786821365356445------------> dt: 102.54ms--------> tokens/sec:2496.53
step: 28 ------> loss: 6.304525852203369------------> dt: 102.29ms--------> tokens/sec:2502.63
step: 29 ------> loss: 6.208136558532715------------> dt: 102.63ms--------> tokens/sec:2494.30
step: 30 ------> loss: 6.2067108154296875------------> dt: 102.64ms--------> tokens/sec:2494.06
step: 31 ------> loss: 6.224257469177246------------> dt: 103.44ms--------> tokens/sec:2474.80
step: 32 ------> loss: 6.189892768859863------------> dt: 103.82ms--------> tokens/sec:2465.74
step: 33 ------> loss: 6.317095756530762------------> dt: 104.06ms--------> tokens/sec:2460.05
step: 34 ------> loss: 6.412139415740967------------> dt: 103.42ms--------> tokens/sec:2475.34
step: 35 ------> loss: 6.284861087799072------------> dt: 103.33ms--------> tokens/sec:2477.59
step: 36 ------> loss: 6.236175060272217------------> dt: 102.57ms--------> tokens/sec:2495.86
step: 37 ------> loss: 6.2227702140808105------------> dt: 102.79ms--------> tokens/sec:2490.41
step: 38 ------> loss: 6.222872734069824------------> dt: 102.47ms--------> tokens/sec:2498.39
step: 39 ------> loss: 6.055088996887207------------> dt: 102.78ms--------> tokens/sec:2490.77
step: 40 ------> loss: 6.2133402824401855------------> dt: 102.91ms--------> tokens/sec:2487.59
step: 41 ------> loss: 5.993866443634033------------> dt: 102.59ms--------> tokens/sec:2495.42
step: 42 ------> loss: 6.105737686157227------------> dt: 103.22ms--------> tokens/sec:2480.23
step: 43 ------> loss: 6.008071422576904------------> dt: 102.57ms--------> tokens/sec:2495.93
step: 44 ------> loss: 5.953313827514648------------> dt: 103.51ms--------> tokens/sec:2473.16
step: 45 ------> loss: 6.178668022155762------------> dt: 104.02ms--------> tokens/sec:2460.98
step: 46 ------> loss: 6.301265716552734------------> dt: 104.22ms--------> tokens/sec:2456.41
step: 47 ------> loss: 6.1616716384887695------------> dt: 103.49ms--------> tokens/sec:2473.73
step: 48 ------> loss: 6.104744911193848------------> dt: 102.86ms--------> tokens/sec:2488.73
step: 49 ------> loss: 6.030588150024414------------> dt: 102.66ms--------> tokens/sec:2493.76
(venv) root@18a2565cd40f:~/project-llm101/practice_nanogpt# python train.py